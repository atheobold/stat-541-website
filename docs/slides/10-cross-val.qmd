---
title: "Cross-Validation and Grid Search"
format:  
  revealjs: 
    theme: [default, style.scss]
editor: source
execute:
  echo: true
---

```{r}
#| include: false
library(reticulate)
use_python("/usr/local/bin/python3")
```

```{python}
#| include: false
#| label: library-load

import numpy as np
import pandas as pd
from plotnine import *
```

# More Information About Final Projects!

# Final Project Presentations

> Will take place on Saturday, March 15 from 1:10pm to 2:30pm, in our classroom! 

## If you are having trouble finding a group to work with

fill out this Google Form and Dr. T will help you! 

::: {.centered}
<iframe src="https://docs.google.com/forms/d/e/1FAIpQLSdFTbTxIwqVEQa3r_nUMtJUa85gLsNpA4qvUo3FOns4GLTGlw/viewform?embedded=true" width="640" height="600" frameborder="0" marginheight="0" marginwidth="0">Loadingâ€¦</iframe>
:::

# The story so far...

## Modeling

We assume some process $f$ is *generating* our target variable:

::: {style="font-size: 125%;"}
**target** = f(**predictors**) + noise
:::

</br>

Our goal is to come up with an approximation of $f$.

## Test Error vs Training Error 

::: {.small}

-   We don't need to know how well our model does on *training data*.

-   We want to know how well it will do on *test data*.

-   In general, test error $>$ training error.
:::

. . .

::: {.small}
> Analogy: A professor posts a practice exam before an exam.

-   If the actual exam is the same as the practice exam, how many points will students miss? That's training error.

-   If the actual exam is different from the practice exam, how many points will students miss? That's test error.

:::

. . .

::: {.small}
> It's always easier to answer questions that you've seen before than questions you haven't seen.

:::

## Modeling Procedure 

[For each model proposed:]{.underline}

::: {.midi}
1. Establish a **pipeline** with **transformers** and a **model**.

2.   **Fit** the pipeline on the **training data** (with known outcome)

3.   **Predict** with the fitted pipeline on **test data** (with known outcome)

4.   **Evaluate** our success; i.e., measure noise "left over"
:::
. . .

[Then:]{.underline}

::: {.midi}
5.  **Select** the best model

6.  **Fit** on *all* the data

7.   **Predict** on any future data (with unknown outcome)
:::

# Linear Regression

## Simple Linear Model

::: {.midi}
We assume that the target ($Y$) is generated from **an equation** of the predictor ($X$), plus random noise ($\epsilon$)

$$Y = \beta_0 + \beta_1 X + \epsilon$$

Goal: Use observations $(x_1, y_1), ..., (x_n, y_n)$ to estimate $\beta_0$ and $\beta_1$.
:::

. . .

::: {.callout-tip}
# What are these parameters???

In Statistics, we use $\beta_0$ to represent the **population** intercept and $\beta_1$ to represent the slope. By "population" we mean the true slope of the line for every observation in the population of interest.
:::

## Measures of Success

What is the "best" choice of $\widehat{\beta}_0$ and $\widehat{\beta}_1$ (the estimates of $\beta_0$ and $\beta_1$)?

-   The ones that are **statistically most justified**, under certain assumptions about $Y$ and $X$?

-   The ones that are "closest to" the observed points?

    -   $|\widehat{y}_i - y_i|$?
    -   $(\widehat{y}_i - y_i)^2$?
    -   $(\widehat{y}_i - y_i)^4$?

## Example: Wine Data

```{python}
#| eval: true
#| label: wine-data-test-train
#| code-line-numbers: false

df = pd.read_csv("https://dlsun.github.io/pods/data/bordeaux.csv")

known = df["year"] < 1981
df_known = df[known]
unknown = df["year"] > 1980
df_unknown = df[unknown]
```

## Price Predicted by Age of Wine

::: {.small}
```{python}
#| code-fold: true
#| eval: false
#| label: age-price-scatterplot-code

from mizani.formatters import currency_format

(
  ggplot(df_known, aes(x = "age", y = "price")) + 
  geom_point() +
  labs(x = "Age of Wine (Years Since 1992)", 
       y = "Price of Wine (in 1992 USD)") +
  scale_y_continuous(labels = currency_format(precision = 0))
)

```
:::

```{python}
#| code-fold: true
#| echo: false
#| label: age-price-scatterplot
#| fig-align: center

from mizani.formatters import currency_format

plot = (
  ggplot(df_known, aes(x = "age", y = "price")) + 
  geom_point() +
  labs(x = "Age of Wine (Years Since 1992)", 
       y = "Price of Wine (in 1992 USD)") +
  scale_y_continuous(labels = currency_format(precision = 0))
)

plot.draw(plot)
```

## "Candidate" Regression Lines

::: {.midi}
Consider five possible regression equations:

$$\text{price} = 25 + 0*\text{age}$$ $$\text{price} = 0 + 10*\text{age}$$ $$\text{price} = 20 + 1*\text{age}$$ $$\text{price} = -40 + 3*\text{age}$$

Which one do you think will be "closest" to the points on the scatterplot?
:::

## "Candidate" Regression Lines

::: {.small}
```{python}
#| eval: false
#| code-fold: true
#| code-line-numbers: false
#| label: candidate-regression-lines-code

(
  ggplot(data = df_known, mapping = aes(x = "age", y = "price")) +
  geom_point() + 
  labs(x = "Age of Wine (Years Since 1992)", 
       y = "Price of Wine (in 1992 USD)") +
  scale_y_continuous(labels = currency_format(precision = 0)) +
  geom_abline(intercept = 25, 
              slope = 0, 
              color = "blue", 
              linetype = "solid") + 
  geom_abline(intercept = 0, 
              slope = 1, 
              color = "orange", 
              linetype = "dashed") + 
  geom_abline(intercept = 20, 
              slope = 1, 
              color = "green") + 
  geom_abline(intercept = -40, 
              slope = 3, 
              color = "magenta")
  )
```
:::

```{python}
#| code-line-numbers: false
#| echo: false
#| label: candidate-regression-lines

plot = (
  ggplot(data = df_known, mapping = aes(x = "age", y = "price")) +
  geom_point() + 
  labs(x = "Age of Wine (Years Since 1992)", 
       y = "Price of Wine (in 1992 USD)") +
  scale_y_continuous(labels = currency_format(precision = 0)) +
  geom_abline(intercept = 25, 
              slope = 0, 
              color = "blue", 
              linetype = "solid") + 
  geom_abline(intercept = 0, 
              slope = 1, 
              color = "orange", 
              linetype = "dashed") + 
  geom_abline(intercept = 20, 
              slope = 1, 
              color = "green", 
              linetype = "dashdot") + 
  geom_abline(intercept = -40, 
              slope = 3, 
              color = "magenta", 
              linetype = "dotted")
  )

plot.draw(plot)
```

## The "best" slope and intercept

-   It's clear that some of these lines are better than others.

-   How to choose the best? **Math!**

-   We'll let the computer do it for us.

::: {.callout-caution}
The estimated slope and intercept are calculated from the **training data** at the `.fit()` step.
:::

## Linear Regression in `sklearn`

::: {.small}
```{python}
#| code-line-numbers: false

from sklearn.linear_model import LinearRegression
from sklearn.pipeline import make_pipeline
```
:::

**Specify**

::: {.small}
```{python}
#| code-line-numbers: false

pipeline = make_pipeline(
    LinearRegression()
    )
```
:::

**Fit**

::: {.small}
```{python}
#| code-line-numbers: false
#| output-location: column

pipeline.fit(
  X = df_known[['age']], 
  y = df_known['price']
  )
```
:::

<!-- ## No scaling? -->

<!-- Insert blurb about why we don't scale with regression! -->

## Estimated Intercept and Slope

::: columns
::: {.column width="45%"}
**Estimated Intercept**

```{python}
#| code-line-numbers: false

(
  pipeline
  .named_steps['linearregression']
  .intercept_
  )
```

:::

::: {.column width="10%"}
:::

::: {.column width="45%"}
**Estimated Slope**

```{python}
#| code-line-numbers: false

(
  pipeline
  .named_steps['linearregression']
  .coef_
  )
```

:::
:::

## Fitting and Predicting

To **predict** from a linear regression, we just plug in the values to the equation...

```{python}
#| code-line-numbers: false
#| label: example-predict-with-coefficients

-0.3 + 1.16 * df_unknown["age"] 
```

## Fitting and Predicting with `.predict()`

To **predict** from a linear regression, we just plug in the values to the equation...

```{python}
#| code-line-numbers: false
#| label: predict-with-sklearn

pipeline.predict(df_unknown[['age']])
```

## Questions to ask yourself 

::: {.midi}
::: {.incremental}
-   **Q:** Is there only one "best" regression line?

-   **A:** No, you can justify many choices of slope and intercept! But there is a generally accepted approach called **Least Squares Regression** that we will always use.

-   **Q:** How do you know which *variables* to include in the equation?

-   **A:** Try them all, and see what predicts best.

-   **Q:** How do you know whether to use *linear regression* or *KNN* to predict?

-   **A:** Try them both, and see what predicts best!

:::
:::

# Cross-Validation

## Resampling methods

::: {.incremental}
-   We saw that a "fair" way to evaluate models was to **randomly split** into **training** and **test** sets.

-   But what if this **randomness** was misleading? *(e.g., a major outlier in one of the sets)*

-   What do we usually do in Statistics to address randomness? Take **many samples** and compute an **average**!

-   A **resampling method** is when we take *many* random test / training splits and *average* the resulting metrics.
:::

## Resampling Method Example

Import all our functions:

```{python}
#| code-line-numbers: "|4-5"
#| label: import-sklearn-functions

from sklearn.compose import make_column_transformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.neighbors import KNeighborsRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
```

## Creating Testing & Training Splits

```{python}
#| code-line-numbers: false
#| label: test-train-wine

X_train, X_test, y_train, y_test = train_test_split(df_known, 
                                                    df_known['price'], 
                                                    test_size = 0.1)
```

. . .

- This creates ***four*** new objects, `X_train`, `X_test`, `y_train`, and 
`y_test`. 
  * Note that the objects are created [in this order]{.underline}!

. . .

- The "testing" data are 10% (0.1) of the total size of the training data 
(`df_known`). 

## Pipeline for Predicting on Test Data

::: {.midi}
**Specify**
:::

::: {.small}
```{python}
#| code-line-numbers: false
#| label: test-train-specify

features = ['summer', 'har', 'sep', 'win', 'age']
ct = make_column_transformer(
  (StandardScaler(), features),
  remainder = "drop"
)

pipeline = make_pipeline(
    ct,
    LinearRegression()
    )
```
:::

::: {.midi}
**Fit for Training Data**
:::

::: {.small}
```{python}
#| code-line-numbers: false
#| label: test-train-fit

pipeline = pipeline.fit(X = X_train, y = y_train)
```
:::

::: {.midi}
**Predict for Test Data**
:::

::: {.small}
```{python}
#| code-line-numbers: false
#| label: test-train-predict

pred_y_test = pipeline.predict(X = X_test)
```
:::

## Estimating Error for Test Data

```{python}
#| code-line-numbers: false
#| label: test-train-error

mean_squared_error(y_test, pred_y_test)
```


## Cross-Validation

-   It makes sense to do test / training many times...

-   But! Remember the original reason for test / training: we don't want to use
the **same data** in *fitting* and *evaluation*.

. . .

**Idea:** Let's make sure that each observation only gets to be in the test set
**once**

. . .

-   **Cross-validation:** Divide the data into 10 random "folds". Each fold gets
a "turn" as the test set.

## Cross-Validation (5-Fold)

![](./images/5-k-fold-pic.png)

## Cross-Validation in `sklearn`

```{python}
#| code-line-numbers: false
#| label: cv-wine

from sklearn.model_selection import cross_val_score

cross_val_score(pipeline, 
                X = df_known, 
                y = df_known['price'], 
                cv = 10)
```

## Cross-Validation in `sklearn`

::: {.midi}
**What are these numbers?**

::: {.incremental}
-   `sklearn` chooses a **default metric** for you based on the model.

-   In the case of regression, the default metric is *R-Squared*.
:::
:::

. . .

::: {.midi}
**Why use negative root mean squared error?**

::: {.incremental}
-   To be consistent! We will **always** want to *maximize* this score. 
  
  + Larger *R-Squared* values explain more of the variance in the response (y). 
  + Larger *negative* RMSE (smaller RMSE) means the "leftover" variance in y 
  is minimized.
:::
:::

## What do we do with these numbers?

```{python}
#| code-line-numbers: false
#| label: cv-wine-2

from sklearn.model_selection import cross_val_score

cvs = cross_val_score(pipeline, 
                      X = df_known, 
                      y = df_known['price'], 
                      cv = 10)
```

Since we have 10 different values, what would you expect us to do?

. . .

Well, this is a statistics class after all. So, you probably guessed we would 
take the mean. 

```{python}
#| code-line-numbers: false
#| label: mean-of-cv-metrics

cvs.mean()
```

## Cross-Validation in `sklearn`

What if you want **MSE**?

```{python}
#| code-line-numbers: "5"
#| label: MSE-instead

cv_scores = cross_val_score(pipeline, 
                            X = df_known, 
                            y = df_known['price'], 
                            cv = 10, 
                            scoring = "neg_mean_squared_error")
cv_scores
```

## Cross-Validation: FAQ 

::: {.small}
::: {.incremental}
-   **Q:** How many cross validations should we do?

-   **A:** It doesn't matter much! Typical choices are 5 or 10. 

-   **A:** Think about the trade-offs: 
    + larger *training sets* = *more accurate models* 
    + smaller *test sets* = *more uncertainty in evaluation*

-   **Q:** What metric should we use?

-   **A:** This is also your choice! What captures your idea of a "successful prediction"? MSE / RMSE is a good default, but you might find other options that
are better!

-   **Q:** I took a statistics class before, and I remember some things like "adjusted R-Squared" or "AIC" for model selection. What about those?

-   **A:** Those are Old School, from a time when computers were not powerful
enough to do cross-validation. Modern data science uses resampling!
:::
:::

# Activity

## Your turn

1.  Use **cross-validation** to choose between Linear Regression and KNN with
k = 7 based on `"neg_mean_squared_error"`, for:

    -   Using all predictors.
    -   Using just winter rainfall and summer temperature.
    -   Using only age.

2.  Re-run #1, but instead use **mean absolute error**. (You will need to look
in the documentation of `cross_val_score()` for this!)

# Tuning with `GridSearchCV()`

## Tuning

-   In previous classes, we tried many different values of $k$ for KNN.

-   We also mentioned using **absolute distance** (Manhattan) instead of
**euclidean distance**.

-   Now, we would like to use **cross-validation** to decide between these
options.

</br> 

`sklearn` provides a nice shortcut for this!

## Initializing `GridSearchCV()`

```{python}
#| echo: false

features = ['summer', 'har', 'sep', 'win', 'age']

ct = make_column_transformer(
  (StandardScaler(), features),
  remainder = "drop"
)

pipeline = make_pipeline(
    ct,
    KNeighborsRegressor()
    )
```

::: {.small}
```{python}
#| code-line-numbers: false
#| label: grid-cv-wine

from sklearn.model_selection import GridSearchCV

grid_cv = GridSearchCV(
    pipeline,
    param_grid = {
        "kneighborsregressor__n_neighbors": range(1, 7),
        "kneighborsregressor__metric": ["euclidean", "manhattan"],
    },
    scoring = "neg_mean_squared_error", 
    cv = 5)
```
:::

::: {.small}
::: {.callout-tip}
# Same column transformer and pipeline!
```{python}
#| eval: false
#| code-line-numbers: false

features = ['summer', 'har', 'sep', 'win', 'age']

ct = make_column_transformer(
  (StandardScaler(), features),
  remainder = "drop"
)

pipeline = make_pipeline(
    ct,
    KNeighborsRegressor()
    )
```
:::
:::

## Fitting `GridSearchCV()`

::: {.small}
```{python}
#| code-line-numbers: false
#| output-location: column

grid_cv.fit(df_known, 
            df_known['price'])
```
:::

. . .

-   **How many different models were fit with this grid?**

## Getting the Cross Validation Results

::: {.midi}
```{python}
#| code-line-numbers: false
#| eval: false

pd.DataFrame(grid_cv.cv_results_)
```
:::

::: {.small}
```{python}
#| code-line-numbers: false
#| echo: false

pd.DataFrame(grid_cv.cv_results_)
```
:::

## What about k and the distances?

::: {.midi}
```{python}
#| eval: false
#| code-line-numbers: false

pd.DataFrame(grid_cv.cv_results_)[['param_kneighborsregressor__metric',
                                   'param_kneighborsregressor__n_neighbors',
                                   'mean_test_score']]
```
:::

::: {.small}
```{python}
#| echo: false
pd.DataFrame(grid_cv.cv_results_)[['param_kneighborsregressor__metric',
                                   'param_kneighborsregressor__n_neighbors',
                                   'mean_test_score']]
```
:::

## What were the best parameters?

```{python}
#| code-line-numbers: false
#| label: best-parameters

grid_cv.best_params_
```

## Model Evaluation

You have now encountered **three types of decisions** for finding your best
model:

::: {.incremental}
1.  Which *predictors* should we include, and how should we preprocess them? (**Feature selection**)

2.  Should we use *Linear Regression* or *KNN* or something else? (**Model selection**)

3.  Which value of $k$ should we use? (**Hyperparameter tuning**)
:::

## Model Evaluation

Think of this like a college sports bracket:

-   Gather all your **candidate pipelines** (combinations of *column transformers* and *model specifications*)

-   **Tune** each pipeline with cross-validation (regional championships!)

-   Determine the **best model type** for each **feature set** (state championships!)

-   Determine the **best pipeline** (national championships!)
