---
title: "K-Nearest-Neighbors"
format:  
  revealjs: 
    theme: [default, style.scss]
editor: source
execute:
  echo: true
---

```{r}
#| include: false
#| label: load-python

library(reticulate)
use_python("/usr/local/bin/python3")
```

```{python}
#| include: false
#| label: import packages

import numpy as np
import pandas as pd
from plotnine import *
```

# Exam 1 Reminders

## What to expect

::: columns
::: {.column width="32%"}
:::{.midi}
**Content** 
:::

::: {.small}
- Everything covered through the end of this week
  * Visualizing numerical & categorical variables
  * Summarizing numerical & categorical variables
  * Distances between observations
  * Preprocessing (scaling / one-hot-encoding) variables

:::
:::

::: {.column width="3%"}
:::

::: {.column width="65%"}
::: {.midi}
**Structure**
:::

::: {.small}
- 80-minutes 
  - First part of class (8:10 - 9:30am; 12:10 - 1:30pm)

- Google Collab Notebook

- Resources you [**can**]{.underline} use:
  * Your own Collab notebooks
  * Any course materials
  * Official Python documentation
  
- Resources you [**can not**]{.underline} use:
  * Other humans
  * Generative AI for anything beyond text completion 
  * Google for anything except to reach Python documentation pages
  
:::
:::
:::

## A Reminder about Code Complexity

> There are always multiple ways to accomplish a given task. 

. . .

In addition to accomplishing the given task, your grade on each problem will also depend on:

- whether your solution uses tools taught in this class, and
- the efficiency of your solution (e.g., did you use a `for` loop when it was not necessary?)

## Efficiency in Preprocessing

::: columns
::: {.column width="65%"}
::: {.small}
```{python}
#| eval: false
#| label: specific-processor
#| code-line-numbers: false

all_transformer = make_column_transformer(
    (StandardScaler(),
     ["abv", "srm", "originalGravity", "ibu"]
     ),
    (OneHotEncoder(sparse_output = False),
     ["isOrganic", "glass", "available"]
     ),
    remainder = "drop")
```

</br>

```{python}
#| eval: false
#| label: general-processor
#| code-line-numbers: false

all_transformer = make_column_transformer(
    (StandardScaler(),
     make_column_selector(dtype_include = np.number)
     ),
    (OneHotEncoder(sparse_output = False),
     make_column_selector(dtype_include = object)
     ),
    remainder = "drop")
```
:::
:::

::: {.column width="5%"}
:::

::: {.column width="30%"}
</br>

What are the ***efficiency*** benefits of making a general preprocessor?
:::
:::


## Data for Exam 1

::: {.midi}
> Your analysis will focus on the qualitites of different types of coffee and how they are related to where / when / how the coffee was grown and processed. 

:::

. . .

![Link to the [raw data)](https://raw.githubusercontent.com/atheobold/data-301/refs/heads/main/exam-materials/coffee_clean.csv)](images/08-exam-data-clue.png){fig-alt="A screenshot of the `coffee.csv` data file you will be analyzing for your Midterm Exam. The preview shows 17 different columns, with two columns highlighted in red---year and altitude. These columns are highlighted because they contain values with some inconsistent formats and may be needed to be cleaned before analysis."}

# The story so far...

## Steps for data analysis

-   **Read** and then **clean** the data
    -   Are there missing values? Will we drop those rows, or replace the
    missing values with something?
    -   Are there *quantitative* variables that Python thinks are *categorical*?
    -   Are there *categorical* variables that Python thinks are *quantitative*?
    -   Are there any *anomalies* in the data that concern you?

## Steps for data analysis (cont'd)

-   **Explore** the data by **visualizing** and **summarizing**.
    -   Different approaches for different combos of *quantitative* and
    *categorical* variables.
    -   Think about *conditional* calculations (split-apply-combine)

## Steps for data analysis (cont'd)

-   Identify a **research question** of interest.

-   Perform **preprocessing** steps

    -   Should we *scale* the quantitative variables?
    -   Should we *one-hot-encode* the categorical variables?
    -   Should we *log-transform* any variables?

-   Measure similarity between **observations** by calculating **distances**.

    -   Which *features* should be included?
    -   Which *distance metric* should we use?

# Predicting Wine Prices

## Data: Wine Qualities

```{python}
#| code-line-numbers: false
#| label: wine-data

df = pd.read_csv("https://dlsun.github.io/pods/data/bordeaux.csv")
df
```

## Data: Wine Quality Variables

::: {.midi}
- `year`: What year the wine was produced
- `price`: Average market price for Bordeaux vintages according to a series of
auctions. 
    + The price is relative to the price of the 1961 vintage, regarded as the
    best one ever recorded.
- `win`: Winter rainfall (in mm)
- `summer`: Average temperature during the summer months (June - August)
- `sep`: Average temperature in the month of September (in Celsius)
- `har`: Rainfall during harvest month(s) (in mm)
- `age`: How old the wine was in 1992 (years since 1992)
:::

## Analysis Goal

> Goal: **Predict** what will be the quality (price) of wines in a
> **future year**.

. . .

Idea: Wines with similar **features** probably have similar **quality**.

::: {.midi}
::: {.incremental}
-   **Inputs**: Summer temperature, harvest rainfall, September temperature, 
winter rainfall, age of wine
    + "Inputs" = "Features" = "Predictors" = "Independent variables"

-   **Output:** Price in 1992
    + "Output" = "Target" = "Dependent variable"
:::
:::

## Similar Wines

Which wines have similar summer temps and winter rainfall to the 1989 vintage?

::: {.small}
```{python}
#| code-fold: true
#| label: scatterplot-temps-rainfall-89-code
#| eval: false
#| code-line-numbers: false

from plotnine import *

(
  ggplot(df, mapping = aes(x = "summer", y = "win")) + 
  geom_point(color = "white") + 
  geom_text(mapping = aes(label = "year")) + 
  theme_bw()
)
```
:::

```{python}
#| label: scatterplot-temps-rainfall-89-plot
#| echo: false

from plotnine import *

plot = (
  ggplot(df, aes(x = "summer", y = "win")) + 
  geom_point(color = "white") + 
  geom_text(aes(label = "year")) + 
  theme_bw()
)

plot.draw(plot)
```

## Predicting 1989

**1989**

::: {.midi}
```{python}
#| label: inspecting-1989
#| code-line-numbers: false

df[df['year'] == 1989]
```
:::

**1990**

::: {.midi}
```{python}
#| label: inspecting-1990
#| code-line-numbers: false

df[df['year'] == 1990]
```
:::

**1976**

::: {.midi}
```{python}
#| label: inspecting-1976
#| code-line-numbers: false

df[df['year'] == 1976]
```
:::

## Training and test data

::: {.midi}
-   The data for which we **know the target** is called the **training data**.

```{python}
#| code-line-numbers: false
#| label: train-1989

known_prices = df['year'] < 1981

df_train = df[known_prices].copy()
```
:::

</br> 

. . .

::: {.midi}
-   The data for which we **don't know the target** (and want to predict it) is
called the **test data**.

```{python}
#| code-line-numbers: false
#| label: test-1989

to_predict = df['year'] == 1989

df_test = df[to_predict].copy()
```
:::

# Testing & Training with `scikitlearn`

## Specify steps

First we make a column transformer...

::: {.midi}
```{python}
#| code-line-numbers: false
#| label: sklearn-transform-columns

from sklearn.compose import make_column_transformer
from sklearn.compose import make_column_selector
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import StandardScaler

preproc = make_column_transformer(
  (StandardScaler(), 
  make_column_selector(dtype_include = np.number)
  ),
  remainder = "drop"
)

features = ['summer', 'har', 'sep', 'win', 'age']
```
:::

. . .

::: {.small}
::: {.callout-tip}
# Why make a general processor?
:::
:::

## Fit the Preprocesser

Then we **fit** it on the **training data**.

::: {.midi}
```{python}
#| code-line-numbers: false
#| label: fit-training
#| output-location: column

preproc.fit(df_train[features])
```
:::

. . .

::: {.midi}
```{python}
#| code-line-numbers: false
#| label: fit-training-mean-variance

preproc.named_transformers_['standardscaler'].mean_
preproc.named_transformers_['standardscaler'].var_
```
:::

## Prep the data

Then we **transform** the **training data** AND the **test data**:

```{python}
#| code-line-numbers: false
#| label: transform-test-train

train_new = preproc.transform(df_train[features])
test_new = preproc.transform(df_test[features])

test_new
```

## What if we had fit on the test data?

::: {.midi}
```{python}
#| code-line-numbers: false
#| label: fit-test
#| output-location: column

preproc.fit(df_test[features])
```
:::

. . .

::: {.midi}
```{python}
#| code-line-numbers: false
#| label: fit-test-mean-variance

preproc.named_transformers_['standardscaler'].mean_
preproc.named_transformers_['standardscaler'].var_
```
:::

## What if we had fit on the test data?

```{python}
#| code-line-numbers: false
#| label: transform-test

preproc.transform(df_test[features])
```

## All together:

::: {.small}
```{python}
#| code-line-numbers: false
#| label: all-scikitlearn-together-specify

preproc = make_column_transformer(
  (StandardScaler(), 
  make_column_selector(dtype_include = np.number)
  ),
  remainder = "drop"
)

features = ['summer', 'har', 'sep', 'win', 'age']
```

```{python}
#| code-line-numbers: false
#| label: all-scikitlearn-together-fit
#| output-location: column

preproc.fit(df_train[features])
```

```{python}
#| code-line-numbers: false
#| label: all-scikitlearn-together-transform

train_new = preproc.transform(df_train[features])
test_new = preproc.transform(df_test[features])
```
:::

## Find the Closest *k*

```{python}
#| code-line-numbers: false
#| label: distances-from-test-train

from sklearn.metrics import pairwise_distances

dists = pairwise_distances(test_new, train_new)
dists
```

## Find the Closest *k*

::: {.midi}
```{python}
#| code-line-numbers: false
#| label: sort-distances

best = (
  dists[0]
  .argsort()
  )
best[0:5]
```
:::

. . .

::: {.midi}
```{python}
#| code-line-numbers: false
#| label: closest-wines

df_train.loc[best]
```
:::

## Predict from the closest *k*

If $k = 1$ ...

```{python}
#| code-line-numbers: false
#| label: k-1

df_train.loc[best[0]]
```

. . .

...we would predict a price of...

```{python}
#| code-line-numbers: false
#| label: k-1-price

df_train.loc[best[0], "price"]
```


## Predict from the closest *k*

If $k = 5$ ...

```{python}
#| code-line-numbers: false
#| label: k-5

df_train.loc[best[0:5]]
```

. . .

</br> 

...we would predict a price of...

```{python}
#| code-line-numbers: false
#| label: k-5-price

df_train.loc[best[0:5], 'price'].mean()
```

## Predict from the closest *k*

If $k = 100$ ...

```{python}
#| code-line-numbers: false
#| label: k-100
#| output: false

df_train.loc[best[0:100]]
```

. . .

</br> 

we would predict a price of...

```{python}
#| code-line-numbers: false
#| label: k-100-price

df_train.loc[best[0:100], 'price'].mean()
```

# Activity 5.1

## Predicting Unknown Prices

Find the predicted 1992 price for *all* the unknown wines (`year` 1981 through
1991), with:

-   $k = 1$

-   $k = 5$

-   $k = 10$

::: {.callout-tip}
# A good place for a function! 

You are performing the same process with different inputs of $k$, so this seems
like a reasonable place to write a function to save you time / reduce errors
from copying and pasting. 
:::

# KNN in `sklearn`

## Specifying

::: {.midi}
```{python}
#| code-line-numbers: false
#| label: sklearn-specify
#| output-location: column

from sklearn.neighbors import KNeighborsRegressor
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(
  preproc,
  KNeighborsRegressor(n_neighbors = 5)
  )
```

::: {.callout-tip}
# Remember the `preproc` we made earlier! 

```{python}
#| eval: false
#| label: preproc-refresher
#| code-line-numbers: false

preproc = make_column_transformer(
  (StandardScaler(), 
  make_column_selector(dtype_include = np.number)
  ),
  remainder = "drop"
)
```
:::
:::

## Fitting

::: {.small}
```{python}
#| code-line-numbers: false
#| label: sklearn-fit
#| output-location: column

pipeline.fit(
  y = df_train['price'], 
  X = df_train[features]
  )
```
:::

## Predicting 

```{python}
#| code-line-numbers: false
#| label: sklearn-predict

pipeline.predict(X = df_test[features])
```

## Activity 2

Find the predicted 1992 price for the *wines with known prices*, with:

-   $k = 1$

-   $k = 5$

-   $k = 10$

How close was each prediction to the right answer?

# K-Nearest-Neighbors Summary

## KNN

::: {.midi}
We have existing observations

$$(X_1, y_1), ... (X_n, y_n)$$ Where $X_i$ is a set of features, and $y_i$ is a
target value.

Given a new observation $X_{new}$, how do we predict $y_{new}$?

1.  Find the $k$ values in $(X_1, ..., X_n)$ that are closest to $X_{new}$

2.  Take the average of the corresponding $y_i$'s to our five closest $X_i$'s.

3.  Predict $\widehat{y}_{new}$ = average of these $y_i$'s
:::

## KNN Big Questions

-   What is our definition of **closest**?

-   What number should we use for $k$?

-   How do we evaluate the **success** of this approach?
