---
title: "Distances Between Observations"
format:  
  revealjs: 
    theme: [default, style.scss]
editor: source
execute:
  echo: true
---

```{r}
#| include: false
library(reticulate)
use_python("/usr/local/bin/python3")
```

```{python}
#| include: false
import numpy as np
import pandas as pd
from plotnine import *
```

# More words about ChatGPT...

## What is my job?

</br> 

::: columns
::: {.column width="40%"}
::: {.fragment}
**Teaching you stuff**

</br> 

(Thoughtfully) choosing what to teach and how to teach it.
::: 
:::

::: {.column width="5%"}
:::

::: {.column width="55%"}
::: {.fragment}
**Assessing what you've learned**

</br> 

What do you understand about the tools I've taught you? 

</br>

::: {.fragment}
This [**is not**]{.underline} the same as assessing if you figured out a way to 
accomplish a given task.
::: 
:::
:::
:::


## Using the tools I teach

::: {.midi}
```{python}
#| eval: false
#| label: individual-datasets
#| code-line-numbers: false
#| echo: true

is_100m = df_bolt["Event"] == "2008 Olympics 100m"
df_100m = df_bolt[is_100m]
one_mean = df_100m["Time"].mean()
one_std = df_100m["Time"].std()

df_bolt["Standardized_Time"] = 0.0
df_bolt.loc[is_100m, "Standardized_Time"] = (df_bolt.loc[is_100m, "Time"] - 
```
:::

. . .

</br>

::: {.midi}
```{python}
#| eval: false
#| label: merge-datasets
#| code-line-numbers: false
#| echo: true

event_stats = df_phelps.groupby('Event')['Time_in_seconds'].agg(['mean', 'std'])
df_phelps = df_phelps.merge(event_stats, on='Event')
df_phelps['Standardized_Time'] = (
(df_phelps['Time_in_seconds'] - df_phelps['mean']) / df_phelps['std']
)

```
:::

## A nice clean, efficient approach

```{python}
#| echo: true
#| eval: false
#| label: nice-solution-standardize
#| code-line-numbers: false

bolt_stats = (
  df_bolt
  .groupby("Event")["Time"]
  .aggregate(["mean", "std"])
  )

standardized_bolt = (
  df_bolt
  .set_index(['Event', 'Athlete'])['Time']
  .subtract(bolt_stats["mean"])
  .divide(bolt_stats["std"])
  )
```


## `lambda` Functions

```{python}
#| eval: false
#| label: lambda-function-times
#| echo: true
#| code-line-numbers: false

phelps_sec = (
  df_phelps["Time"]
  .str.split(":")
  .apply(lambda x: float(x[0])*60 + float(x[1]))
)
```

. . .

</br>

```{python}
#| eval: false
#| label: my-approach-times
#| echo: true
#| code-line-numbers: false

df_phelps[["Minutes", "Seconds"]] = df_phelps["Time"].str.split(":")
df_phelps["Time_New"] = (
  df_phelps["Minutes"].astype(float) * 60 +
  df_phelps["Seconds"].astype(float)
  )
```

## A loop is often not necessary

```{python}
#| eval: false
#| label: loop-for-times
#| echo: true
#| code-line-numbers: false

split_times = df_phelps["Time"].str.split(":")
seconds = []

for time in split_times:
  minute = int(time[0])
  second = float(time[1])
  seconds.append((minute * 60) + second)
  df_phelps["Seconds"] = seconds

```

## When to make a function?

::: {.small}
```{python}
#| eval: false
#| label: time-function
#| echo: true
#| code-line-numbers: false

def time_to_secs(time_str):
  mins, secs = time_str.split(':')
  return float(mins) * 60 + float(secs)

df_phelps['time_secs'] = df_phelps['Time'].apply(time_to_secs)
```
:::

. . .

</br> 

::: {.small}
```{python}
#| eval: false
#| label: simpsons-function
#| echo: true
#| code-line-numbers: false

def calculate_simpson_index(values, position):
    
    # Convert values to a Pandas Series, ensuring they are strings
    values_series = pd.Series(values).astype(str)
    
    # Extract the specified character based on the position
    extracted_character = values_series.str[position]
    
    # Calculate the frequency of each character
    character_counts = extracted_character.value_counts(normalize=True)
    
    # Compute the Simpson's Index
    simpson_index = 1 - sum(character_counts ** 2)
    
    return simpson_index
```
:::

# The story so far...

## Summarizing

-   **One categorical variable:** marginal distribution

-   **Two categorical variables:** joint and conditional distributions

-   **One quantitative variable:** mean, median, variance, standard deviation.

-   **One quantitative, one categorical:** mean, median, and std dev across 
groups (`groupby()`, *split-apply-combine*)

-   **Two quantitative variables:** z-scores, correlation

## Visualizing

-   **One categorical variable:** bar plot or column plot

-   **Two categorical variables:** stacked bar plot, side-by-side bar plot, or
stacked percentage bar plot

-   **One quantitative variable:** histogram, density plot, or boxplot

-   **One quantitative, one categorical:** overlapping densities, side-by-side
boxplots, or facetting

-   **Two quantitative variables:** scatterplot

# Today's data: House prices

## Ames house prices

::: {.small}
```{python}
#| code-line-numbers: false
#| label: ames-housing-data

df = pd.read_table("https://datasci112.stanford.edu/data/housing.tsv",
                    sep = "\\t")
df.head()
```


::: {.callout-tip}
# `read_table` not `read_csv`

This is a `tsv` file (tab separated values), so we need to use a different 
function to read in our data! The `sep` argument allows you to specify the 
delimiter the file uses, but you can also allow the system to autodetect the
delimiter. 
:::
:::

## How does house size relate to number of bedrooms?

. . .

::: {.small}
```{python}
#| code-line-numbers: false
#| code-fold: true
#| label: size-bedroom-scatterplot-code
#| eval: false

(
  ggplot(df, mapping = aes(x = "Gr Liv Area", y = "Bedroom AbvGr"))  + 
  geom_point() +
  labs(x = "Total Living Area", 
       y = "Number of Bedrooms")
 )
```
:::

```{python}
#| code-line-numbers: false
#| label: size-bedroom-scatterplot
#| fig-align: center
#| fig-width: 2.5
#| echo: false

plot = (
  ggplot(df, mapping = aes(y = "Gr Liv Area", x = "Bedroom AbvGr"))  + 
  geom_point() +
  labs(y = "Total Living Area (Square Feet)", 
       x = "Number of Bedrooms")
 )

plot.draw(plot)
```

## How does house size relate to number of bedrooms?

What *statistic* would you calculate?

. . .

```{python}
#| code-line-numbers: false
#| label: bedroom-size-correlation

df[["Gr Liv Area", "Bedroom AbvGr"]].corr()
```

# Measuring Similarity with Distance

## Similarity

How might we answer the question, "Are these two houses similar?"

::: columns
::: {.column width="45%"}
::: {.small}
```{python}
#| code-line-numbers: false
#| label: house-1

df.loc[1707, ["Gr Liv Area", "Bedroom AbvGr"]]
```
:::
:::

::: {.column width="10%"}
:::

::: {.column width="45%"}
::: {.small}
```{python}
#| code-line-numbers: false
#| label: house-2

df.loc[290, ["Gr Liv Area", "Bedroom AbvGr"]]
```
:::
:::
:::

## Distance

The **distance** between the **two observations** is:

$$ \sqrt{ (2956 - 2650)^2 + (5 - 6)^2} = 306 $$

. . .

... what does this number mean? Not much! 

But we can use it to **compare** sets of houses and find houses that appear to 
be the **most similar**.

## Another House to Consider

::: columns
::: {.column width="45%"}
```{python}
#| code-line-numbers: false
#| label: house-1-again

df.loc[1707, ["Gr Liv Area", "Bedroom AbvGr"]]

```

:::

::: {.column width="10%"}
:::

::: {.column width="45%"}
```{python}
#| code-line-numbers: false
#| label: house-3

df.loc[291, ["Gr Liv Area", "Bedroom AbvGr"]]
```

:::
:::

. . .

$$ \sqrt{ (2956 - 1666)^2 + (5 - 3)^2} = 1290 $$ 

Thus, house 1707 is **more similar** to house 290 than to house 291.

## [Lecture Activity Part 1]{.large}

> Complete Part One of the activity linked in Canvas.

```{r}
#| echo: false
library(countdown)
countdown(minutes = 10, left = "0")
```

# Scaling / Standardizing

## House 160 seems more similar...

::: {.small}
```{python}
#| code-fold: true
#| code-line-numbers: false
#| label: coloring-specific-houses-code
#| eval: false

(
  ggplot(df, mapping = aes(y = "Gr Liv Area", x = "Bedroom AbvGr")) + 
  geom_point(color = "lightgrey") + 
  geom_point(df.loc[[1707]], color = "red", size = 2, shape = 17) + 
  geom_point(df.loc[[160]], color = "blue", size = 2) + 
  geom_point(df.loc[[2336]], color = "green", size = 2) + 
  theme_bw() +
  labs(y = "Total Living Area (Square Feet)", 
       x = "Number of Bedrooms")
)
```
:::

```{python}
#| label: coloring-specific-houses
#| echo: false
#| fig-align: center

plot = (
  ggplot(df, mapping = aes(y = "Gr Liv Area", x = "Bedroom AbvGr")) + 
  geom_point(color = "lightgrey") + 
  geom_point(df.loc[[1707]], color = "red", size = 5, shape = "x") + 
  geom_point(df.loc[[160]], color = "blue", size = 2) + 
  geom_point(df.loc[[2336]], color = "green", size = 2) + 
  theme_bw() +
  labs(y = "Total Living Area (Square Feet)", 
       x = "Number of Bedrooms")
)

plot.draw(plot)
```

## ...even if we zoom in...

::: {.small}
```{python}
#| code-fold: true
#| code-line-numbers: false
#| label: zooming-in-x-code
#| eval: false

(
  ggplot(df, mapping = aes(y = "Gr Liv Area", x = "Bedroom AbvGr")) + 
  geom_point(color = "lightgrey") + 
  geom_point(df.loc[[1707]], color = "red", size = 5, shape = "x") + 
  geom_point(df.loc[[160]], color = "blue", size = 2) + 
  geom_point(df.loc[[2336]], color = "green", size = 2) + 
  theme_bw() +
  labs(y = "Total Living Area (Square Feet)", 
       x = "Number of Bedrooms") +
  scale_y_continuous(limits = (2500, 3500))
)
```
:::

```{python}
#| label: zooming-in-x
#| echo: false
#| fig-align: center

plot = (
  ggplot(df, mapping = aes(y = "Gr Liv Area", x = "Bedroom AbvGr")) + 
  geom_point(color = "lightgrey") + 
  geom_point(df.loc[[1707]], color = "red", size = 5, shape = "x") + 
  geom_point(df.loc[[160]], color = "blue", size = 2) + 
  geom_point(df.loc[[2336]], color = "green", size = 2) + 
  theme_bw() +
  labs(y = "Total Living Area (Square Feet)", 
       x = "Number of Bedrooms") +
  scale_y_continuous(limits = (2500, 3500))
)

plot.draw(plot)
```

## ...but not if we put the axes on the same **scale**!

::: {.small}
```{python}
#| code-fold: true
#| code-line-numbers: false
#| label: x-y-on-same-scale-code
#| eval: false

(
  ggplot(df, aes(y = "Gr Liv Area", x = "Bedroom AbvGr")) + 
  geom_point(color = "lightgrey") + 
  geom_point(df.loc[[1707]], color = "red", size = 5, shape = "x") + 
  geom_point(df.loc[[160]], color = "blue", size = 2) + 
  geom_point(df.loc[[2336]], color = "green", size = 2) + 
  theme_bw() +
  labs(y = "Total Living Area (Square Feet)", 
       x = "Number of Bedrooms") +
  scale_y_continuous(limits = (2900, 3000)) +
  scale_x_continuous(limits = (0, 100))
)
```
:::

```{python}
#| label: x-y-on-same-scale
#| echo: false
#| fig-align: center

plot = (
  ggplot(df, aes(y = "Gr Liv Area", x = "Bedroom AbvGr")) + 
  geom_point(color = "lightgrey") + 
  geom_point(df.loc[[1707]], color = "red", size = 5, shape = "x") + 
  geom_point(df.loc[[160]], color = "blue", size = 2) + 
  geom_point(df.loc[[2336]], color = "green", size = 2) + 
  theme_bw() +
  labs(y = "Total Living Area (Square Feet)", 
       x = "Number of Bedrooms") +
  scale_y_continuous(limits = (2900, 3000)) +
  scale_x_continuous(limits = (0, 100))
)

plot.draw(plot)
```

## Scaling

We need to make sure our features are on the same **scale** before we can
use **distances** to measure **similarity**.

::: {.callout-tip}
# Standardizing 

subtract the mean, divide by the standard deviation
:::

<!-- -   In this case, the **mean** doesn't really matter. (why?) -->

## Scaling

::: {.small}
```{python}
#| code-line-numbers: false

df['size_scaled'] = (df['Gr Liv Area'] - df['Gr Liv Area'].mean()) / df['Gr Liv Area'].std()
df['bdrm_scaled'] = (df['Bedroom AbvGr'] - df['Bedroom AbvGr'].mean()) / df['Bedroom AbvGr'].std()
```
:::

::: {.small}
```{python}
#| code-fold: true
#| code-line-numbers: false
#| label: scatterplot-of-standardized-vars-code
#| eval: false

(
  ggplot(df, aes(y = "size_scaled", x = "bdrm_scaled")) + 
  geom_point(color = "lightgrey") + 
  geom_point(df.loc[[1707]], color = "red", size = 5, shape = "x") + 
  geom_point(df.loc[[160]], color = "blue", size = 2) + 
  geom_point(df.loc[[2336]], color = "green", size = 2) + 
  theme_bw() +
  labs(y = "Total Living Area (Standardized)", 
       x = "Number of Bedrooms (Standardized)") 
)
```
:::

```{python}
#| label: scatterplot-of-standardized-vars
#| echo: false
#| fig-align: center

plot = (
  ggplot(df, aes(y = "size_scaled", x = "bdrm_scaled")) + 
  geom_point(color = "lightgrey") + 
  geom_point(df.loc[[1707]], color = "red", size = 5, shape = "x") + 
  geom_point(df.loc[[160]], color = "blue", size = 2) + 
  geom_point(df.loc[[2336]], color = "green", size = 2) + 
  theme_bw() +
  labs(y = "Total Living Area (Standardized)", 
       x = "Number of Bedrooms (Standardized)") 
)

plot.draw(plot)
```

## [Lecture Activity Part 2]{.large}

> Complete Part Two of the activity linked in Canvas.

```{r}
#| echo: false
#| label: countdown-part-2

countdown(minutes = 10, left = "0")
```

# Scikit-learn

## Scikit-learn

-   `scikit-learn` is a library for **machine learning** and **modeling**

-   We will use it a lot in this class!

-   For now, we will use it as a shortcut for *scaling* and for
*computing distances*

. . .

-   The philosophy of `sklearn` is:

    -   **specify** your analysis
    -   **fit** on the data to prepare the analysis
    -   **transform** the data

## Specify

```{python}
#| code-line-numbers: false

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler
```

**No calculations have happened yet!**

## Fit

The `scaler` object "learns" the means and standard deviations.

::: {.small}
```{python}
#| code-line-numbers: false

df_orig = df[['Gr Liv Area', 'Bedroom AbvGr']]
scaler.fit(df_orig)
scaler.mean_
scaler.scale_
```
:::

. . .

We still have not altered the data at all!

## Transform

```{python}
#| code-line-numbers: false

df_scaled = scaler.transform(df_orig)
df_scaled
```

## sklearn, numpy, and pandas

::: {.midi}
-   By default, `sklearn` functions return `numpy` objects.

-   This is sometimes annoying, maybe we want to plot things after scaling.

-   Solution: remake it, with the original column names.
:::

. . .

::: {.small}
```{python}
#| code-line-numbers: false

pd.DataFrame(df_scaled, columns = df_orig.columns)
```
:::

## Distances with sklearn

```{python}
#| code-line-numbers: false

from sklearn.metrics import pairwise_distances

pairwise_distances(df_scaled[[1707]], df_scaled)
```

## Finding the Most Similar

::: columns
::: {.column width="48%"}
::: {.small}
```{python}
#| code-line-numbers: false

dists = pairwise_distances(df_scaled[[1707]], 
                           df_scaled)
dists.argsort()
```
:::
:::

::: {.column width="2%"}
:::

::: {.column width="50%"}
::: {.small}
```{python}
#| code-line-numbers: false

best = (
  dists
  .argsort()
  .flatten()
  [1:10]
  )
  
df_orig.iloc[best]
```
:::
:::
:::

## [Lecture Activity Part 3]{.large}

> Complete Part Three of the activity linked in Canvas.

```{r}
#| echo: false
#| label: countdown-part-3

library(countdown)
countdown(minutes = 10, left = "0")
```

# Alternatives

## Other scaling

-   Standardization $$x_i \leftarrow \frac{x_i - \bar{X}}{\text{sd}(X)}$$

-   Min-Max Scaling $$x_i \leftarrow \frac{x_i - \text{min}(X)}{\text{max}(X) - \text{min}(X)}$$

## Other distances

-   Euclidean ($\ell_2$)

    $$\sqrt{\sum_{j=1}^m (x_j - x'_j)^2}$$

-   Manhattan ($\ell_1$)

    $$\sum_{j=1}^m |x_j - x'_j|$$

## [Lecture Activity Part 4]{.large}

> Complete Part Four of the activity linked in Canvas.

```{r}
#| echo: false
#| label: countdown-part-4

countdown(minutes = 10, left = "0")
```

# Takeaways

## Takeaways

-   We measure similarity between **observations** by calculating **distances**.

-   It is important that all our **features** be on the same **scale** for
distances to be meaningful.

-   We can use `scikit-learn` functions to **fit** and **transform** data, and
to compute pairwise distances.

-   There are many options of ways to *scale* data; most common is
**standardizing**

-   There are many options of ways to *measure distances*; most common is
**Euclidean distance**.
