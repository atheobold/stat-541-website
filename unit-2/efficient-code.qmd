---
title: "Week 8:  Writing Fast Code"
editor: source
execute: 
  echo: false
  warning: false
  message: false
---

```{r}
#| label: setup
#| include: false

source("../scripts/window_dressing.R")

library(tidyverse)
library(tictoc)
library(microbenchmark)

options(digits = 2)
```

<!-- You might be wondering to yourself - why bother with matrices, when we have so many nice tools in R for data frames? -->

<!-- It turns out that the tradeoff for the niceness of R's data frames is in speed: it takes longer to do basic calculations when you have variables of many different types. -->

In this module, you'll learn a bit about how to make your code faster in general.

```{r, results = "asis", echo = FALSE}
make_summary_table(vid_time = "10 min", 
                   reading_time = "40 min", 
                   work_time = "10 min", 
                   canvas_activities = 4)
```

# Code speed

Fortunately, we stand on the shoulders of giants. Many skilled computer scientists have put a lot of time and effort into writing R functions that run as fast as possible. For you, most of the work to speed up code is simply finding the right packages to rely on.

However, there are a few baseline principles that can get you a long way. If your code takes a long time to run, the reason is often one of these:

-   You are doing a very large computation, relative to the power of your computer.
-   You are repeating a slow process many times.
-   You are trying to do simple things, but on a very large object.

To speed up the code, without deep knowledge of computer algorithms and inner workings, you can sometimes come up with clever ways to avoid these pitfalls.

First, though: as you start thinking about writing faster code, you'll need a way to check whether your improvements actually sped up the code.

```{r}
#| results: asis
#| label: tic-toc-microbenchmark-packages

req_read("The tictoc and microbenchmark packages", url = 'https://www.jumpingrivers.com/blog/timing-in-r/')
```

<!-- ```{r} -->

<!-- #| results: asis -->

<!-- #| label: bench-package -->

<!-- req_read("The bench package", url = 'https://www.tidyverse.org/blog/2018/06/bench-1.0.1/') -->

<!-- ``` -->

<!-- ```{r} -->

<!-- #| results: asis -->

<!-- #| label: tic-toc-microbenchmark-packages -->

<!-- req_read("lobstr::obj_size", url = 'https://lobstr.r-lib.org/reference/obj_size.html',  -->

<!--          type = "Optional") -->

<!-- ``` -->

# Tip 1: Avoid larger calculations and memory demands

## Save smaller data frames, if you are going to use them many times

Consider the following dataset:

```{r}
#| echo: true
#| message: false
#| label: load-data

library(fivethirtyeight)

glimpse(congress_age)
```

Suppose we want to do some exploratory analysis on these data. Let's use the `tic()` and `toc()` functions from the **tictoc** packages to check how much time it takes to run this code:

```{r}
#| results: false
#| message: false
#| fig-show: hide
#| echo: true
#| label: exploratory-analysis

tic()

congress_age %>%
  filter(chamber == "senate") %>%
  summarize(mean(age))

congress_age %>%
  filter(chamber == "senate") %>%
  ggplot() + 
  geom_histogram(mapping = aes(x = age))

congress_age %>%
  filter(chamber == "senate") %>%
  slice_max(n = 5, order_by = age)

```

```{r}
#| echo: true
toc()
```

These are all three reasonable things to do, and they can't be done in the same pipeline. But wait - we just made R do the process of subsetting to only Senators three separate times!

```{r, results = FALSE}
#| echo: true

tic()
  congress_age %>%
    filter(chamber == "senate")
```

```{r}
#| echo: true
toc()
```

Instead, how about we filter the data once, save it into an object, and call on that object:

```{r}
#| echo: true
#| fig-show: hide
#| message: false
#| results: false

tic()

 senate <- congress_age %>%
  filter(chamber == "senate")

senate %>%
  summarize(mean(age))

senate %>%
  ggplot() + 
  geom_histogram(mapping = aes(x = age))


senate %>%
  slice_max(n = 5, order_by = age)
```

```{r}
#| echo: true

toc()
```

<!-- ### Be smart about order of matrix operations -->

<!-- Think carefully about how matrix multiplication works. You want to avoid doing multiplications between big matrices as much as you can. -->

<!-- ```{r, results = "asis", echo = FALSE} -->

<!-- checkin("Order of matrix multiplication") -->

<!-- ``` -->

<!-- Suppose you have two matrices and a (column) vector that you would like to multiply together: ${\bf A} {\bf B}{\bf y}$. -->

<!-- **Question 1:** What is the faster way to do this? Use `tictoc` or `bench::mark()` or similar to time the steps. -->

<!-- *(You may want to make these matrices a bit smaller, if you are on a personal laptop, to avoid annoying R crashes. You can work your way up to bigger ones until you see a noticeable difference in measured speed.)* -->

<!-- ```{r, eval = FALSE} -->

<!-- A <- matrix(rnorm(100000), nrow = 500, ncol = 200) -->

<!-- B <- matrix(rnorm(180000), nrow = 200, ncol = 900) -->

<!-- y <- matrix(rnorm(900)) -->

<!-- ## a -->

<!-- res <- (A %*% B) %*% y -->

<!-- ## b -->

<!-- res <- A %*% (B %*% y) -->

<!-- ``` -->

<!-- **Question 2:** Intuitively, why was the faster one faster? Think about the matrix calculations that are being done at each step. -->

# Tip 2: Avoid repeating slow steps

## Avoid for-loops

For-loops are deathly slow in R. If you absolutely must iterate over a process, rely on the `apply()` or `map()` function families. *These will typically be close to the same speed, depending on the situation.*

```{r}
#| echo: true
#| cache: true
#| warning: false
#| label: compare-loops-map-apply

loop_func <- function(df){
  
  # initialize the type object
  type <- rep(NA, ncol(df))
  
  for(i in 1:ncol(df)){
    
    # store the class of the ith column of df into the ith entry of type 
    type[i] <- class(df[,i])
  }
  
  return(type)
}

map_func <- function(df){
  
  type <- map_chr(df, class)
  
  return(type)

  }

dat <- as.data.frame(
  matrix(1,
         nrow = 5,
         ncol = 100000)
  )

microbenchmark::microbenchmark(loop_func(dat),
                               map_func(dat),
                               times = 2)
```

## Use vectorized functions

Better even than `apply()` or `map()` is not to iterate at all! Writing vectorized functions is tough, but do-able in many cases. Here's an example:

```{r}
#| results: asis
#| label: vectorizing-functions

req_vid("Simple Vectorizing Example", url = 'https://www.youtube.com/embed/ShewFCbPHv8', embed = FALSE)
```

Better yet - rely on built-in functions in other packages to do it for you!

## Allocate memory in advance

A trick we use very often in coding is to "tack on" values to the end of a vector or list as we iterate through something. However, this is actually very slow! If you know how long your list is going to end up, consider pre-creating that list.

```{r}
#| echo: true

do_stuff_allocate <- function(reps) {
  results <- vector(length = reps)
  for (i in 1:reps) {
    results[i] <- i
  }
  return(results)
}

do_stuff_tackon <- function(reps) {
   results <- c()
  for (i in 1:reps) {
    results <- c(results, i)
  }
  return(results)
}

microbenchmark::microbenchmark(
  do_stuff_allocate(1000),
  do_stuff_tackon(1000)
)

```

# Tip 3: Use faster existing functions

Because R has so many packages, there are often many functions to perform the same task. Not all functions are created equal!

<!-- ### matrix operations -->

<!-- A few other functions you might find useful: `rowSums()` and `colSums()`, -->

<!-- `rowMeans()` and `colMeans()`. -->

<!-- ```{r, results = "asis", echo = FALSE} -->

<!-- checkin("Cross products") -->

<!-- ``` -->

<!-- For the two approaches below, which is faster? -->

<!-- *You may want to start with a smaller matrix, then gradually increase the size until you see a noticeable time difference on your computer.* -->

<!-- ```{r, eval = FALSE} -->

<!-- my_big_matrix <- matrix(rnorm(100000), nrow = 100, ncol = 1000) -->

<!-- ## a -->

<!-- res <- t(my_big_matrix) %*% my_big_matrix -->

<!-- ## b -->

<!-- res <- crossprod(my_big_matrix) -->

<!-- ``` -->

## data.table

For speeding up work with data frames, no package is better than `data.table`!

```{r, results = "asis", echo = FALSE}
req_vid("5 minute intro to data.table", url = 'https://www.youtube.com/embed/uueVddWwbkk', embed = FALSE)
```

```{r, results = "asis", echo = FALSE}
req_read("data.table cheatsheet", url = 'https://pbs.twimg.com/media/Dwoq0ZQXQAE5lJt.jpg')
```

```{r, results = "asis", echo = FALSE}
checkin("data.table")
```

Here is some `dplyr` code. Re-write it in `data.table` syntax. Then perform a speed test to see which one is faster, and by how much.

```{r}
#| echo: true

congress_age %>%
  group_by(state) %>%
  summarize(mean_age = mean(age))

```

# Tip 4: Only improve what needs improving

> “We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%. A good programmer will not be lulled into complacency by such reasoning, \[they\] will be wise to look carefully at the critical code; but only after that code has been identified.” — Donald Knuth.

Speed and computational efficiency are important, but there can be a tradeoff. If you make a small speed improvement, but your code becomes overly complex, confusing, or difficult to edit and test it's not worth it!

Also, consider your time and energy as the programmer. Suppose you have a function that takes 30-minutes to run. It relies on a subfunction that takes 30-seconds to run. Should you really spend your efforts making the subfunction take only 10-seconds?

The art of finding the slow bits of your code where you can make the most improvement is called **profiling**

```{r, results = "asis", echo = FALSE}
req_read("Profiling", url = 'http://adv-r.had.co.nz/Profiling.html#profiling')
```

*For this reading, I strongly encourage you to skim. The code aspects are mostly more complicated that we expect in this class; but the general overview of the principles behind speeding up code is helpful.*

*In your work, you will most likely "profile" manually, by `tictoc`-ing bits of your code and so forth, not by these fancy methods.*

```{r, results = "asis", echo = FALSE}
checkin("Profiling")
```

**Question 1:** Why is `vapply()` faster than `sapply()`?

a.  It specifies the output type.
b.  It specifies the input type.
c.  It performs fewer safety checks.
d.  It performs fewer iterations.

**Question 2:** Why is `mean(x)` slower than `sum(x)/length(x)`?

a.  `sum()` is vectorized, while `mean()` is not.
b.  A mean is a smaller number than a sum, so it needs less memory.
c.  `mean()` calculates the mean twice just in case.
d.  `mean()` was written in an old version of R.

**Question 3:** What is the name of the site where you can search packages by topic?

a.  CRANberries
b.  CRAN search algorithm
c.  CRAN archive network
d.  CRAN task views

## Optional: Super advanced mode

The following are some suggestions of ways you can level-up your code speed. These are outside the scope of this class, but you are welcome to explore them!

```{r, results = "asis", echo = FALSE}
extra_rec("")
```

-   [Dive deep into the R efficiency rabbit hole...](https://csgillespie.github.io/efficientR/)
-   [Parallelize your computations](http://zevross.com/blog/2019/02/12/dramatically-speed-up-your-r-purrr-functions-with-the-furrr-package/)
-   [Make your code run in C++ with Rcpp](http://adv-r.had.co.nz/Rcpp.html#rcpp)
-   Learn to use [recursion](https://techvidvan.com/tutorials/recursion-in-r/)
-   Learn more about [memory allocation](https://adv-r.hadley.nz/names-values.html) and [garbage collecting](http://adv-r.had.co.nz/memory.html) in R.
